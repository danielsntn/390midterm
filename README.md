# STOR 390 Final Project
## Red-Teaming in LLMs

The following paper is an analysis on **Curiosity-Driven Red-Teaming in Large Language Models** by Hong et. al (2024). The research paper focuses on the use of red-teaming in LLMs, which is when testers purposely attempt to trigger harmful responses from the model. 
To increase efficiency, researchers have moved away from human red-team and started using red-team LLMs. As traditional red-team LLMs are built using usually fall short of optimization, Hong et. al started using a method called curiosity-driven reinforcement learning to 
for better optimization.

**Note: My final RMD file is titled midterm.rmd**
